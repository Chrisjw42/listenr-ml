{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "5abea3ac-4fa5-4c4f-893f-7f2afa49e523",
    "_kg_hide-output": true,
    "_uuid": "337e0950ca948be32d5d881c1a3c675ccf7ac523",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1001)\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import sklearn\n",
    "import dask.dataframe as dd\n",
    "from dask import array\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import librosa.display\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv1D, Dropout, Dense, MaxPooling1D, Flatten, Conv2D, MaxPooling2D\n",
    "from keras import Sequential\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client, wait, progress\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:52899\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>12</li>\n",
       "  <li><b>Cores: </b>12</li>\n",
       "  <li><b>Memory: </b>16.90 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:52899' processes=12 cores=12>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = pd.read_parquet(\"train_meta\")\n",
    "test_meta = pd.read_parquet(\"test_meta\")\n",
    "val_meta = pd.read_parquet(\"val_meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stft_2d(y, sr, hop_length=32):\n",
    "    D = librosa.stft(y, hop_length=hop_length, n_fft=1024)\n",
    "    spec = librosa.amplitude_to_db(D,ref=np.max)\n",
    "    df = pd.DataFrame(spec / 80)\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_in_seconds(y,sr):\n",
    "    return len(y)/sr\n",
    "\n",
    "def create_samples_from_fn(idx, fn, return_data=False, save_data=True):\n",
    "    \"\"\"\n",
    "    Also trims silence from a file\n",
    "    \"\"\"\n",
    "    desired_sr = 22050\n",
    "    try:\n",
    "        # Load, resample if needed\n",
    "        y, sr = librosa.load(fn)\n",
    "    except:\n",
    "        return None\n",
    "    if sr != desired_sr:\n",
    "        y = librosa.core.resample(y, sr, desired_sr)\n",
    "        sr = desired_sr\n",
    "\n",
    "    # Standard scaling\n",
    "    standardScaler = StandardScaler()    \n",
    "    y = standardScaler.fit_transform(y.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "    length = get_length_in_seconds(y, sr)\n",
    "\n",
    "    #ax.plot(y, alpha=0.3)\n",
    "    #print(f\"{np.round(length, 4)}\")\n",
    "    # Trim silence\n",
    "    y_trimmed = librosa.effects.trim(y, top_db=12.5)[0]\n",
    "    length_trimmed = get_length_in_seconds(y_trimmed, sr)\n",
    "    #print(f\"{np.round(length_trimmed, 4)}\")\n",
    "    #plt.plot(y_trimmed, alpha=0.5)\n",
    "\n",
    "    # Split into chunks\n",
    "    chunk_len = int(sr / 2) # .5 seconds\n",
    "    end = len(y_trimmed) - (len(y_trimmed) % chunk_len)\n",
    "    n_chunks = int(end / chunk_len)\n",
    "    if n_chunks == 0:\n",
    "        return None\n",
    "    y_trimmed_chunks = np.split(y_trimmed[:end], n_chunks)    \n",
    "    y_trimmed_chunks = [get_stft_2d(ch, desired_sr) for ch in y_trimmed_chunks]\n",
    "    \n",
    "    for i in range(len(y_feature_chunks)):\n",
    "        res = pd.DataFrame(y_feature_chunks[i])\n",
    "        res.columns = res.columns.astype(str)\n",
    "        res[\"sample\"]=\"{}-{}\".format(idx, i) # Store the\n",
    "        if save_data:\n",
    "            res.to_parquet(\"../data/listenr-ml/preprocessed_3/{}-{}.parquet\".format(idx, i))\n",
    "    if return_data:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_data(metadata):\n",
    "    \"\"\"\n",
    "    Save all preprocessed (and eventually feature engineered samples)\n",
    "    \"\"\"\n",
    "    n = len(metadata)\n",
    "    \n",
    "    futures = []\n",
    "\n",
    "    for idx, data in metadata.iterrows():\n",
    "        futures.append(client.submit(create_samples_from_fn, idx, data[\"filepath\"], return_data=False, save_data=True,\n",
    "                                    key=idx))\n",
    "    return futures\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# futures = establish_data(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_one(fn):\n",
    "#     res = pd.read_parquet(\"../data/listenr-ml/preprocessed_2/{}\".format(fn))\n",
    "#     return res\n",
    "# res = load_one(\"0-0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def get_scaler(client, metadata):\n",
    "    \"\"\"\n",
    "    Load all samples and train a standardscaler\n",
    "    \"\"\"\n",
    "    folder = \"../data/listenr-ml/preprocessed_3/\"\n",
    "    basenames = os.listdir(folder)\n",
    "    fns = [\"../data/listenr-ml/preprocessed_3/{}\".format(x) for x in basenames]\n",
    "    \n",
    "    sScaler = StandardScaler()\n",
    "\n",
    "    # k is chunk size, i is iterator\n",
    "    k = 500\n",
    "    i = 0\n",
    "    \n",
    "    while (i*k) < len(fns):\n",
    "        \n",
    "        start = i*k\n",
    "        end = min((i+1)*k, len(fns))\n",
    "        \n",
    "        print(start,end)\n",
    "        i+= 1\n",
    "        futures = [client.submit(pd.read_parquet, x) for x in fns[start:end]]\n",
    "        x = pd.concat(client.gather(futures))\n",
    "        sScaler.partial_fit(x[x.columns[:-1]])\n",
    "    \n",
    "    with open(\"2DScaler.p\", \"wb\") as fp:\n",
    "        pickle.dump(sScaler, fp)\n",
    "        \n",
    "    return sScaler\n",
    "    \n",
    "    # First, train a standard scaler on all elements\n",
    "    df = dd.read_parquet(\"{}*.parquet\".format(folder)).compute()\n",
    "    \n",
    "    gc.collect()\n",
    "    client.restart()\n",
    "\n",
    "    print(df.shape)\n",
    "    print(\"scaling\")\n",
    "    # Transform to standard scale, then ignore the sample name column\n",
    "    sScaler.fit(df[df.columns[:-1]])\n",
    "    \n",
    "    with open(\"2DScaler.p\", \"wb\") as fp:\n",
    "        pickle.dump(sScaler, fp)\n",
    "    \n",
    "    return sScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = get_scaler(client, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator StandardScaler from version 0.20.0 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "def scale_a_sample(fn, scaler):\n",
    "    df = pd.read_parquet(fn)\n",
    "    #Transform to standard scale, then ignore the sample name column\n",
    "    X = pd.DataFrame(scaler.transform(df[df.columns[:-1]]))\n",
    "    X.columns = [str(x) for x in X.columns]\n",
    "    b = os.path.basename(fn)\n",
    "    X.to_parquet(\"../data/listenr-ml/preprocessed_3_scaled/{}\".format(b))\n",
    "    \n",
    "def scale_all_samples(scaler):\n",
    "    \"\"\"\n",
    "    Load all samples into keras-digestible format\n",
    "    \"\"\"\n",
    "    folder = \"../data/listenr-ml/preprocessed_3/\"\n",
    "    \n",
    "    basenames = os.listdir(folder)\n",
    "        \n",
    "    fns = [\"../data/listenr-ml/preprocessed_3/{}\".format(x) for x in basenames]\n",
    "    # Get the original sample's metada for each of these chunks\n",
    "    indices = [int(x.split(\"-\")[0]) for x in basenames]\n",
    "    futures = []\n",
    "    for fn in fns:\n",
    "        futures.append(client.submit(scale_a_sample, fn, scaler))\n",
    "    return futures\n",
    "\n",
    "with open(\"2DScaler.p\", \"rb\") as fp:\n",
    "    scaler = pickle.load(fp)\n",
    "    \n",
    "# ff = scale_all_samples(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_input(path):\n",
    "    return pd.read_parquet(path).values.reshape((513, 345, 1))\n",
    "    \n",
    "def get_output(idx, metadata):\n",
    "    return metadata[metadata.columns[-6:]].loc[idx].values\n",
    "\n",
    "def batch_gen(metadata, batch_size = 8):\n",
    "  \n",
    "    while True:\n",
    "        # Select files (paths/indices) for the batch\n",
    "        indices = np.random.choice(a = metadata.index, \n",
    "                                     size = batch_size)\n",
    "        \n",
    "        data = [(int(x.split(\"-\")[0]), f\"../data/listenr-ml/preprocessed_3_scaled/{x}\") for x in \n",
    " os.listdir(\"../data/listenr-ml/preprocessed_3_scaled/\") if \n",
    " int(x.split(\"-\")[0]) in indices]\n",
    "        \n",
    "        batch_input = []\n",
    "        batch_output = []\n",
    "\n",
    "        # Read in each input, perform preprocessing and get labels\n",
    "        for idx, path in data:\n",
    "            \n",
    "            input_ = get_input(path)\n",
    "            output = get_output(idx,metadata)\n",
    "\n",
    "            batch_input += [input_]\n",
    "            batch_output += [output]\n",
    "\n",
    "        # Return a tuple of (input,output) to feed the network\n",
    "\n",
    "        batch_x = np.array(batch_input)\n",
    "        batch_y = np.array(batch_output)\n",
    "\n",
    "        yield(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G-G-G-GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2D_model_test(input_shape, n_classes, dropout, \n",
    "                        base_dense, n_filters, n_conv, filter_size=10, extra_dense=False):\n",
    "    model = Sequential()\n",
    "    \n",
    "    for i in range(n_conv):\n",
    "        model.add(Conv2D(n_filters, filter_size,\n",
    "                                padding='valid',\n",
    "                                input_shape=input_shape,\n",
    "                                activation=\"relu\"))\n",
    "        model.add(MaxPooling2D(padding=\"same\", pool_size=(filter_size)))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        filter_size -= 3\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(base_dense, activation=\"relu\"))\n",
    "    model.add(Dropout(dropout))    \n",
    "\n",
    "    if extra_dense:\n",
    "        model.add(Dense(int(base_dense / 2), activation=\"relu\"))\n",
    "        model.add(Dropout(dropout))    \n",
    "\n",
    "    model.add(Dense(n_classes, activation=\"softmax\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = build_2D_model_test((513,345, 1), 6,  \n",
    "                         dropout=0.1, \n",
    "                         n_conv=3,\n",
    "                         base_dense=64,\n",
    "                         n_filters=128,\n",
    "                         extra_dense=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 2 64 64 False\n",
      "Epoch 1/50\n",
      " - 124s - loss: 1.8114 - acc: 0.1882 - val_loss: 1.7805 - val_acc: 0.2180\n",
      "Epoch 2/50\n",
      " - 122s - loss: 1.7812 - acc: 0.2021 - val_loss: 1.7653 - val_acc: 0.2281\n",
      "Epoch 3/50\n",
      " - 121s - loss: 1.7784 - acc: 0.1922 - val_loss: 1.7801 - val_acc: 0.1885\n",
      "Epoch 4/50\n",
      " - 121s - loss: 1.7736 - acc: 0.1947 - val_loss: 1.7696 - val_acc: 0.2299\n",
      "Epoch 5/50\n",
      " - 122s - loss: 1.7695 - acc: 0.1768 - val_loss: 1.7698 - val_acc: 0.2133\n",
      "Epoch 6/50\n",
      " - 121s - loss: 1.7675 - acc: 0.1925 - val_loss: 1.7752 - val_acc: 0.2113\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 7s 492ms/step\n",
      "0 [1.771514255252085, 0.20711974396964108]\n",
      "0.5 2 64 32 False\n",
      "Epoch 1/50\n",
      " - 122s - loss: 1.8104 - acc: 0.1952 - val_loss: 1.7812 - val_acc: 0.2039\n",
      "Epoch 2/50\n",
      " - 118s - loss: 1.7820 - acc: 0.1818 - val_loss: 1.7721 - val_acc: 0.2102\n",
      "Epoch 3/50\n",
      " - 121s - loss: 1.7784 - acc: 0.1932 - val_loss: 1.7645 - val_acc: 0.2179\n",
      "Epoch 4/50\n",
      " - 118s - loss: 1.7784 - acc: 0.1916 - val_loss: 1.7741 - val_acc: 0.1962\n",
      "Epoch 5/50\n",
      " - 121s - loss: 1.7719 - acc: 0.2025 - val_loss: 1.7801 - val_acc: 0.1980\n",
      "Epoch 6/50\n",
      " - 119s - loss: 1.7725 - acc: 0.1853 - val_loss: 1.7739 - val_acc: 0.2048\n",
      "Epoch 7/50\n",
      " - 122s - loss: 1.7683 - acc: 0.1915 - val_loss: 1.7705 - val_acc: 0.2053\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00007: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 343ms/step\n",
      "1 [1.8024314455952204, 0.1418439731371741]\n",
      "0.5 2 64 16 False\n",
      "Epoch 1/50\n",
      " - 123s - loss: 1.8125 - acc: 0.1821 - val_loss: 1.7808 - val_acc: 0.2098\n",
      "Epoch 2/50\n",
      " - 120s - loss: 1.7797 - acc: 0.1898 - val_loss: 1.7738 - val_acc: 0.2141\n",
      "Epoch 3/50\n",
      " - 120s - loss: 1.7756 - acc: 0.1889 - val_loss: 1.7692 - val_acc: 0.1968\n",
      "Epoch 4/50\n",
      " - 121s - loss: 1.7825 - acc: 0.1814 - val_loss: 1.7762 - val_acc: 0.2149\n",
      "Epoch 5/50\n",
      " - 120s - loss: 1.7697 - acc: 0.1986 - val_loss: 1.7695 - val_acc: 0.2131\n",
      "Epoch 6/50\n",
      " - 119s - loss: 1.7718 - acc: 0.1839 - val_loss: 1.7713 - val_acc: 0.2157\n",
      "Epoch 7/50\n",
      " - 121s - loss: 1.7784 - acc: 0.1881 - val_loss: 1.7684 - val_acc: 0.2400\n",
      "Epoch 8/50\n",
      " - 120s - loss: 1.7714 - acc: 0.1812 - val_loss: 1.7711 - val_acc: 0.2044\n",
      "Epoch 9/50\n",
      " - 121s - loss: 1.7744 - acc: 0.1977 - val_loss: 1.7687 - val_acc: 0.2442\n",
      "Epoch 10/50\n",
      " - 120s - loss: 1.7716 - acc: 0.1858 - val_loss: 1.7624 - val_acc: 0.2210\n",
      "Epoch 11/50\n",
      " - 122s - loss: 1.7776 - acc: 0.1805 - val_loss: 1.7700 - val_acc: 0.2319\n",
      "Epoch 12/50\n",
      " - 119s - loss: 1.7717 - acc: 0.2012 - val_loss: 1.7726 - val_acc: 0.2008\n",
      "Epoch 13/50\n",
      " - 120s - loss: 1.7771 - acc: 0.1996 - val_loss: 1.7673 - val_acc: 0.2130\n",
      "Epoch 14/50\n",
      " - 120s - loss: 1.7731 - acc: 0.1992 - val_loss: 1.7732 - val_acc: 0.2047\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00014: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 340ms/step\n",
      "2 [1.769693795687113, 0.11217948634368487]\n",
      "0.5 2 32 64 False\n",
      "Epoch 1/50\n",
      " - 123s - loss: 1.8107 - acc: 0.1920 - val_loss: 1.7685 - val_acc: 0.2204\n",
      "Epoch 2/50\n",
      " - 121s - loss: 1.7922 - acc: 0.1856 - val_loss: 1.7803 - val_acc: 0.1973\n",
      "Epoch 3/50\n",
      "FAILED\n",
      "OOM when allocating tensor with shape[35,64,504,336] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node training/Adam/gradients/max_pooling2d_1/MaxPool_grad/MaxPoolGrad}} = MaxPoolGrad[T=DT_FLOAT, data_format=\"NCHW\", ksize=[1, 1, 10, 10], padding=\"SAME\", strides=[1, 1, 10, 10], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_1/Relu, max_pooling2d_1/MaxPool, training/Adam/gradients/AddN_3)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0.5 2 32 32 False\n",
      "Epoch 1/50\n",
      " - 124s - loss: 1.8183 - acc: 0.1841 - val_loss: 1.7864 - val_acc: 0.2249\n",
      "Epoch 2/50\n",
      " - 122s - loss: 1.7881 - acc: 0.1913 - val_loss: 1.7763 - val_acc: 0.2183\n",
      "Epoch 3/50\n",
      " - 120s - loss: 1.7907 - acc: 0.1871 - val_loss: 1.7818 - val_acc: 0.2169\n",
      "Epoch 4/50\n",
      " - 121s - loss: 1.7771 - acc: 0.1869 - val_loss: 1.7791 - val_acc: 0.2038\n",
      "Epoch 5/50\n",
      " - 121s - loss: 1.7789 - acc: 0.2075 - val_loss: 1.7781 - val_acc: 0.1820\n",
      "Epoch 6/50\n",
      " - 121s - loss: 1.7762 - acc: 0.1856 - val_loss: 1.7744 - val_acc: 0.2208\n",
      "Epoch 7/50\n",
      " - 122s - loss: 1.7695 - acc: 0.2049 - val_loss: 1.7676 - val_acc: 0.2104\n",
      "Epoch 8/50\n",
      " - 122s - loss: 1.7578 - acc: 0.2337 - val_loss: 1.7517 - val_acc: 0.2531\n",
      "Epoch 9/50\n",
      " - 121s - loss: 1.7534 - acc: 0.2162 - val_loss: 1.7616 - val_acc: 0.2238\n",
      "Epoch 10/50\n",
      " - 123s - loss: 1.7680 - acc: 0.2251 - val_loss: 1.7522 - val_acc: 0.2234\n",
      "Epoch 11/50\n",
      " - 120s - loss: 1.7559 - acc: 0.2187 - val_loss: 1.7732 - val_acc: 0.2216\n",
      "Epoch 12/50\n",
      " - 122s - loss: 1.7415 - acc: 0.2449 - val_loss: 1.7418 - val_acc: 0.2359\n",
      "Epoch 13/50\n",
      " - 123s - loss: 1.7511 - acc: 0.2263 - val_loss: 1.7333 - val_acc: 0.2575\n",
      "Epoch 14/50\n",
      " - 121s - loss: 1.7394 - acc: 0.2443 - val_loss: 1.7336 - val_acc: 0.2695\n",
      "Epoch 15/50\n",
      " - 121s - loss: 1.7289 - acc: 0.2358 - val_loss: 1.7236 - val_acc: 0.2727\n",
      "Epoch 16/50\n",
      " - 121s - loss: 1.7260 - acc: 0.2507 - val_loss: 1.7125 - val_acc: 0.2986\n",
      "Epoch 17/50\n",
      " - 118s - loss: 1.7332 - acc: 0.2630 - val_loss: 1.7227 - val_acc: 0.2781\n",
      "Epoch 18/50\n",
      " - 122s - loss: 1.7205 - acc: 0.2611 - val_loss: 1.7185 - val_acc: 0.2642\n",
      "Epoch 19/50\n",
      " - 124s - loss: 1.7112 - acc: 0.2697 - val_loss: 1.7235 - val_acc: 0.2730\n",
      "Epoch 20/50\n",
      " - 121s - loss: 1.7083 - acc: 0.2689 - val_loss: 1.7226 - val_acc: 0.2600\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00020: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 325ms/step\n",
      "3 [1.7003192681155792, 0.3767123311655979]\n",
      "0.5 2 32 16 False\n",
      "Epoch 1/50\n",
      " - 123s - loss: 1.8214 - acc: 0.1968 - val_loss: 1.7800 - val_acc: 0.2017\n",
      "Epoch 2/50\n",
      " - 117s - loss: 1.7769 - acc: 0.1961 - val_loss: 1.7757 - val_acc: 0.2066\n",
      "Epoch 3/50\n",
      " - 123s - loss: 1.7765 - acc: 0.1889 - val_loss: 1.7706 - val_acc: 0.2091\n",
      "Epoch 4/50\n",
      " - 120s - loss: 1.7665 - acc: 0.2053 - val_loss: 1.7776 - val_acc: 0.1938\n",
      "Epoch 5/50\n",
      " - 120s - loss: 1.7681 - acc: 0.1933 - val_loss: 1.7691 - val_acc: 0.2415\n",
      "Epoch 6/50\n",
      " - 119s - loss: 1.7671 - acc: 0.1931 - val_loss: 1.7527 - val_acc: 0.2354\n",
      "Epoch 7/50\n",
      " - 120s - loss: 1.7539 - acc: 0.2227 - val_loss: 1.7657 - val_acc: 0.2276\n",
      "Epoch 8/50\n",
      " - 121s - loss: 1.7549 - acc: 0.2095 - val_loss: 1.7474 - val_acc: 0.2319\n",
      "Epoch 9/50\n",
      " - 123s - loss: 1.7605 - acc: 0.2064 - val_loss: 1.7624 - val_acc: 0.2163\n",
      "Epoch 10/50\n",
      " - 119s - loss: 1.7438 - acc: 0.2173 - val_loss: 1.7620 - val_acc: 0.2062\n",
      "Epoch 11/50\n",
      " - 120s - loss: 1.7457 - acc: 0.2288 - val_loss: 1.7591 - val_acc: 0.2324\n",
      "Epoch 12/50\n",
      " - 121s - loss: 1.7417 - acc: 0.2327 - val_loss: 1.7618 - val_acc: 0.2106\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00012: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 319ms/step\n",
      "4 [1.77974639357182, 0.14736842435964367]\n",
      "0.5 1 64 64 False\n",
      "Epoch 1/50\n",
      " - 126s - loss: 2.8695 - acc: 0.1827 - val_loss: 1.7848 - val_acc: 0.2083\n",
      "Epoch 2/50\n",
      "FAILED\n",
      "OOM when allocating tensor with shape[35,64,504,336] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/kernel/read)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[{{node loss/mul/_97}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_681_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5 1 64 32 False\n",
      "Epoch 1/50\n",
      " - 124s - loss: 1.9197 - acc: 0.1882 - val_loss: 1.7858 - val_acc: 0.2266\n",
      "Epoch 2/50\n",
      " - 122s - loss: 1.7855 - acc: 0.1979 - val_loss: 1.7810 - val_acc: 0.2179\n",
      "Epoch 3/50\n",
      " - 122s - loss: 1.7783 - acc: 0.2030 - val_loss: 1.7777 - val_acc: 0.2207\n",
      "Epoch 4/50\n",
      " - 123s - loss: 1.7755 - acc: 0.1948 - val_loss: 1.7753 - val_acc: 0.2137\n",
      "Epoch 5/50\n",
      " - 120s - loss: 1.7757 - acc: 0.1775 - val_loss: 1.7833 - val_acc: 0.1939\n",
      "Epoch 6/50\n",
      " - 122s - loss: 1.7682 - acc: 0.1989 - val_loss: 1.7708 - val_acc: 0.2333\n",
      "Epoch 7/50\n",
      " - 123s - loss: 1.7762 - acc: 0.1920 - val_loss: 1.7700 - val_acc: 0.2090\n",
      "Epoch 8/50\n",
      " - 123s - loss: 1.7730 - acc: 0.1915 - val_loss: 1.7724 - val_acc: 0.2019\n",
      "Epoch 9/50\n",
      " - 122s - loss: 1.7758 - acc: 0.1876 - val_loss: 1.7669 - val_acc: 0.2103\n",
      "Epoch 10/50\n",
      " - 121s - loss: 1.7651 - acc: 0.2031 - val_loss: 1.7736 - val_acc: 0.1930\n",
      "Epoch 11/50\n",
      " - 121s - loss: 1.7707 - acc: 0.2043 - val_loss: 1.7614 - val_acc: 0.2255\n",
      "Epoch 12/50\n",
      " - 124s - loss: 1.7681 - acc: 0.1994 - val_loss: 1.7686 - val_acc: 0.2076\n",
      "Epoch 13/50\n",
      " - 125s - loss: 1.7779 - acc: 0.1926 - val_loss: 1.7654 - val_acc: 0.2013\n",
      "Epoch 14/50\n",
      " - 122s - loss: 1.7745 - acc: 0.2009 - val_loss: 1.7701 - val_acc: 0.2309\n",
      "Epoch 15/50\n",
      " - 123s - loss: 1.7688 - acc: 0.2169 - val_loss: 1.7730 - val_acc: 0.1887\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00015: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 328ms/step\n",
      "5 [1.7447353359191649, 0.2580645172346023]\n",
      "0.5 1 64 16 False\n",
      "Epoch 1/50\n",
      " - 125s - loss: 1.8840 - acc: 0.1769 - val_loss: 1.7811 - val_acc: 0.2324\n",
      "Epoch 2/50\n",
      " - 121s - loss: 1.7816 - acc: 0.1997 - val_loss: 1.7810 - val_acc: 0.2051\n",
      "Epoch 3/50\n",
      " - 121s - loss: 1.7789 - acc: 0.1886 - val_loss: 1.7706 - val_acc: 0.2561\n",
      "Epoch 4/50\n",
      " - 122s - loss: 1.7782 - acc: 0.2079 - val_loss: 1.7730 - val_acc: 0.2332\n",
      "Epoch 5/50\n",
      " - 120s - loss: 1.7735 - acc: 0.1770 - val_loss: 1.7733 - val_acc: 0.2065\n",
      "Epoch 6/50\n",
      " - 121s - loss: 1.7758 - acc: 0.1947 - val_loss: 1.7687 - val_acc: 0.2262\n",
      "Epoch 7/50\n",
      " - 120s - loss: 1.7675 - acc: 0.1905 - val_loss: 1.7683 - val_acc: 0.2057\n",
      "Epoch 8/50\n",
      " - 123s - loss: 1.7703 - acc: 0.1951 - val_loss: 1.7749 - val_acc: 0.1908\n",
      "Epoch 9/50\n",
      " - 121s - loss: 1.7729 - acc: 0.1898 - val_loss: 1.7692 - val_acc: 0.2016\n",
      "Epoch 10/50\n",
      " - 122s - loss: 1.7670 - acc: 0.2050 - val_loss: 1.7749 - val_acc: 0.2066\n",
      "Epoch 11/50\n",
      " - 123s - loss: 1.7672 - acc: 0.1880 - val_loss: 1.7662 - val_acc: 0.2237\n",
      "Epoch 12/50\n",
      " - 123s - loss: 1.7669 - acc: 0.1857 - val_loss: 1.7734 - val_acc: 0.2014\n",
      "Epoch 13/50\n",
      " - 121s - loss: 1.7771 - acc: 0.2044 - val_loss: 1.7805 - val_acc: 0.1996\n",
      "Epoch 14/50\n",
      " - 122s - loss: 1.7819 - acc: 0.1930 - val_loss: 1.7695 - val_acc: 0.2042\n",
      "Epoch 15/50\n",
      " - 123s - loss: 1.7729 - acc: 0.2056 - val_loss: 1.7683 - val_acc: 0.2333\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00015: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 319ms/step\n",
      "6 [1.7862858166059943, 0.15358361835772674]\n",
      "0.5 1 32 64 False\n",
      "Epoch 1/50\n",
      " - 127s - loss: 1.9000 - acc: 0.1990 - val_loss: 1.7854 - val_acc: 0.2114\n",
      "Epoch 2/50\n",
      " - 125s - loss: 1.7805 - acc: 0.1907 - val_loss: 1.7746 - val_acc: 0.2102\n",
      "Epoch 3/50\n",
      " - 124s - loss: 1.7792 - acc: 0.2027 - val_loss: 1.7757 - val_acc: 0.1984\n",
      "Epoch 4/50\n",
      " - 125s - loss: 1.7763 - acc: 0.2151 - val_loss: 1.7699 - val_acc: 0.2259\n",
      "Epoch 5/50\n",
      " - 124s - loss: 1.7731 - acc: 0.2137 - val_loss: 1.7693 - val_acc: 0.2114\n",
      "Epoch 6/50\n",
      " - 127s - loss: 1.7773 - acc: 0.1832 - val_loss: 1.7696 - val_acc: 0.2006\n",
      "Epoch 7/50\n",
      "FAILED\n",
      "OOM when allocating tensor with shape[34,64,504,336] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node training/Adam/gradients/max_pooling2d_1/MaxPool_grad/MaxPoolGrad}} = MaxPoolGrad[T=DT_FLOAT, data_format=\"NCHW\", ksize=[1, 1, 10, 10], padding=\"SAME\", strides=[1, 1, 10, 10], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_1/Relu, max_pooling2d_1/MaxPool, training/Adam/gradients/AddN_2)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0.5 1 32 32 False\n",
      "Epoch 1/50\n",
      " - 126s - loss: 1.8553 - acc: 0.1891 - val_loss: 1.7876 - val_acc: 0.1969\n",
      "Epoch 2/50\n",
      " - 123s - loss: 1.7839 - acc: 0.1833 - val_loss: 1.7781 - val_acc: 0.2188\n",
      "Epoch 3/50\n",
      " - 124s - loss: 1.7790 - acc: 0.2024 - val_loss: 1.7753 - val_acc: 0.2138\n",
      "Epoch 4/50\n",
      " - 121s - loss: 1.7749 - acc: 0.1843 - val_loss: 1.7720 - val_acc: 0.2106\n",
      "Epoch 5/50\n",
      " - 123s - loss: 1.7728 - acc: 0.1918 - val_loss: 1.7727 - val_acc: 0.2222\n",
      "Epoch 6/50\n",
      " - 121s - loss: 1.7703 - acc: 0.1829 - val_loss: 1.7711 - val_acc: 0.1930\n",
      "Epoch 7/50\n",
      " - 121s - loss: 1.7737 - acc: 0.1757 - val_loss: 1.7662 - val_acc: 0.2070\n",
      "Epoch 8/50\n",
      " - 123s - loss: 1.7674 - acc: 0.1789 - val_loss: 1.7745 - val_acc: 0.2068\n",
      "Epoch 9/50\n",
      " - 122s - loss: 1.7706 - acc: 0.1954 - val_loss: 1.7725 - val_acc: 0.2156\n",
      "Epoch 10/50\n",
      " - 123s - loss: 1.7751 - acc: 0.2055 - val_loss: 1.7691 - val_acc: 0.2250\n",
      "Epoch 11/50\n",
      " - 122s - loss: 1.7714 - acc: 0.1835 - val_loss: 1.7686 - val_acc: 0.2078\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00011: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 312ms/step\n",
      "7 [1.7921609238880436, 0.22996515996157085]\n",
      "0.5 1 32 16 False\n",
      "Epoch 1/50\n",
      " - 126s - loss: 1.8332 - acc: 0.2042 - val_loss: 1.7848 - val_acc: 0.2104\n",
      "Epoch 2/50\n",
      " - 121s - loss: 1.7835 - acc: 0.1831 - val_loss: 1.7791 - val_acc: 0.2270\n",
      "Epoch 3/50\n",
      " - 124s - loss: 1.7806 - acc: 0.2029 - val_loss: 1.7732 - val_acc: 0.2213\n",
      "Epoch 4/50\n",
      " - 120s - loss: 1.7807 - acc: 0.1999 - val_loss: 1.7748 - val_acc: 0.2167\n",
      "Epoch 5/50\n",
      " - 120s - loss: 1.7807 - acc: 0.1833 - val_loss: 1.7750 - val_acc: 0.2064\n",
      "Epoch 6/50\n",
      " - 122s - loss: 1.7804 - acc: 0.1847 - val_loss: 1.7791 - val_acc: 0.2104\n",
      "Epoch 7/50\n",
      " - 123s - loss: 1.7745 - acc: 0.1714 - val_loss: 1.7683 - val_acc: 0.2087\n",
      "Epoch 8/50\n",
      " - 120s - loss: 1.7743 - acc: 0.1934 - val_loss: 1.7795 - val_acc: 0.1781\n",
      "Epoch 9/50\n",
      " - 123s - loss: 1.7708 - acc: 0.1938 - val_loss: 1.7700 - val_acc: 0.2196\n",
      "Epoch 10/50\n",
      " - 123s - loss: 1.7798 - acc: 0.1666 - val_loss: 1.7730 - val_acc: 0.2105\n",
      "Epoch 11/50\n",
      " - 122s - loss: 1.7783 - acc: 0.1654 - val_loss: 1.7681 - val_acc: 0.2208\n",
      "Epoch 12/50\n",
      " - 123s - loss: 1.7690 - acc: 0.2087 - val_loss: 1.7671 - val_acc: 0.2318\n",
      "Epoch 13/50\n",
      " - 122s - loss: 1.7723 - acc: 0.2106 - val_loss: 1.7751 - val_acc: 0.2155\n",
      "Epoch 14/50\n",
      " - 122s - loss: 1.7684 - acc: 0.2078 - val_loss: 1.7697 - val_acc: 0.2051\n",
      "Epoch 15/50\n",
      " - 122s - loss: 1.7693 - acc: 0.1894 - val_loss: 1.7724 - val_acc: 0.2081\n",
      "Epoch 16/50\n",
      " - 120s - loss: 1.7758 - acc: 0.1858 - val_loss: 1.7714 - val_acc: 0.1862\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00016: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 320ms/step\n",
      "8 [1.7482210097696957, 0.19798657675617493]\n",
      "0.3 2 64 64 False\n",
      "Epoch 1/50\n",
      " - 128s - loss: 1.8036 - acc: 0.1761 - val_loss: 1.7587 - val_acc: 0.2012\n",
      "Epoch 2/50\n",
      " - 123s - loss: 1.7867 - acc: 0.1989 - val_loss: 1.7821 - val_acc: 0.2077\n",
      "Epoch 3/50\n",
      " - 124s - loss: 1.7700 - acc: 0.2264 - val_loss: 1.7473 - val_acc: 0.2317\n",
      "Epoch 4/50\n",
      "FAILED\n",
      "OOM when allocating tensor with shape[38,64,504,336] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/kernel/read)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[{{node loss/mul/_113}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_851_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.3 2 64 32 False\n",
      "Epoch 1/50\n",
      " - 124s - loss: 1.8001 - acc: 0.1846 - val_loss: 1.7851 - val_acc: 0.1966\n",
      "Epoch 2/50\n",
      " - 125s - loss: 1.7848 - acc: 0.1987 - val_loss: 1.7607 - val_acc: 0.2199\n",
      "Epoch 3/50\n",
      " - 124s - loss: 1.7769 - acc: 0.1975 - val_loss: 1.7803 - val_acc: 0.2150\n",
      "Epoch 4/50\n",
      " - 121s - loss: 1.7701 - acc: 0.2042 - val_loss: 1.7765 - val_acc: 0.2105\n",
      "Epoch 5/50\n",
      " - 123s - loss: 1.7737 - acc: 0.1775 - val_loss: 1.7680 - val_acc: 0.2139\n",
      "Epoch 6/50\n",
      " - 122s - loss: 1.7784 - acc: 0.1828 - val_loss: 1.7743 - val_acc: 0.2150\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 322ms/step\n",
      "9 [1.7945411991992783, 0.1279461314080139]\n",
      "0.3 2 64 16 False\n",
      "Epoch 1/50\n",
      " - 125s - loss: 1.7947 - acc: 0.1948 - val_loss: 1.7737 - val_acc: 0.2041\n",
      "Epoch 2/50\n",
      " - 122s - loss: 1.7606 - acc: 0.2360 - val_loss: 1.7568 - val_acc: 0.1682\n",
      "Epoch 3/50\n",
      " - 124s - loss: 1.7371 - acc: 0.2459 - val_loss: 1.7720 - val_acc: 0.2015\n",
      "Epoch 4/50\n",
      " - 124s - loss: 1.7266 - acc: 0.2517 - val_loss: 1.6888 - val_acc: 0.2967\n",
      "Epoch 5/50\n",
      " - 123s - loss: 1.6818 - acc: 0.2781 - val_loss: 1.6731 - val_acc: 0.3082\n",
      "Epoch 6/50\n",
      " - 123s - loss: 1.6683 - acc: 0.2798 - val_loss: 1.6694 - val_acc: 0.3156\n",
      "Epoch 7/50\n",
      " - 124s - loss: 1.6285 - acc: 0.3263 - val_loss: 1.6238 - val_acc: 0.3190\n",
      "Epoch 8/50\n",
      " - 122s - loss: 1.5940 - acc: 0.3400 - val_loss: 1.6258 - val_acc: 0.3076\n",
      "Epoch 9/50\n",
      " - 122s - loss: 1.5853 - acc: 0.3401 - val_loss: 1.6126 - val_acc: 0.3042\n",
      "Epoch 10/50\n",
      " - 120s - loss: 1.5706 - acc: 0.3553 - val_loss: 1.6078 - val_acc: 0.3225\n",
      "Epoch 11/50\n",
      " - 122s - loss: 1.5569 - acc: 0.3631 - val_loss: 1.6431 - val_acc: 0.3021\n",
      "Epoch 12/50\n",
      " - 123s - loss: 1.5429 - acc: 0.3566 - val_loss: 1.5849 - val_acc: 0.3470\n",
      "Epoch 13/50\n",
      " - 123s - loss: 1.5148 - acc: 0.3687 - val_loss: 1.5676 - val_acc: 0.3340\n",
      "Epoch 14/50\n",
      " - 123s - loss: 1.4954 - acc: 0.3883 - val_loss: 1.5761 - val_acc: 0.3488\n",
      "Epoch 15/50\n",
      " - 122s - loss: 1.4817 - acc: 0.3912 - val_loss: 1.5584 - val_acc: 0.3588\n",
      "Epoch 16/50\n",
      " - 123s - loss: 1.4665 - acc: 0.3949 - val_loss: 1.5800 - val_acc: 0.3351\n",
      "Epoch 17/50\n",
      " - 124s - loss: 1.4431 - acc: 0.4215 - val_loss: 1.5875 - val_acc: 0.3329\n",
      "Epoch 18/50\n",
      " - 125s - loss: 1.4151 - acc: 0.4240 - val_loss: 1.5759 - val_acc: 0.3239\n",
      "Epoch 19/50\n",
      " - 123s - loss: 1.4432 - acc: 0.4149 - val_loss: 1.5787 - val_acc: 0.3539\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00019: early stopping\n",
      "15/15 [==============================] - ETA: 10 - ETA: 6 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 353ms/step\n",
      "10 [1.5734535435702177, 0.3489932928909391]\n",
      "0.3 2 32 64 False\n",
      "Epoch 1/50\n",
      "FAILED\n",
      "OOM when allocating tensor with shape[37,64,504,336] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/kernel/read)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0.3 2 32 32 False\n",
      "Epoch 1/50\n",
      " - 126s - loss: 1.8005 - acc: 0.1996 - val_loss: 1.7745 - val_acc: 0.2104\n",
      "Epoch 2/50\n",
      " - 122s - loss: 1.7838 - acc: 0.2025 - val_loss: 1.7774 - val_acc: 0.2228\n",
      "Epoch 3/50\n",
      " - 123s - loss: 1.7774 - acc: 0.1927 - val_loss: 1.7762 - val_acc: 0.1938\n",
      "Epoch 4/50\n",
      " - 124s - loss: 1.7740 - acc: 0.1804 - val_loss: 1.7645 - val_acc: 0.1689\n",
      "Epoch 5/50\n",
      " - 123s - loss: 1.7755 - acc: 0.1795 - val_loss: 1.7687 - val_acc: 0.1941\n",
      "Epoch 6/50\n",
      " - 123s - loss: 1.7672 - acc: 0.1965 - val_loss: 1.7744 - val_acc: 0.2038\n",
      "Epoch 7/50\n",
      " - 125s - loss: 1.7728 - acc: 0.2113 - val_loss: 1.7676 - val_acc: 0.2331\n",
      "Epoch 8/50\n",
      " - 122s - loss: 1.7755 - acc: 0.1951 - val_loss: 1.7716 - val_acc: 0.1915\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00008: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 317ms/step\n",
      "11 [1.7622878829776618, 0.16376306742222052]\n",
      "0.3 2 32 16 False\n",
      "Epoch 1/50\n",
      " - 126s - loss: 1.7992 - acc: 0.1846 - val_loss: 1.7843 - val_acc: 0.2210\n",
      "Epoch 2/50\n",
      " - 122s - loss: 1.7797 - acc: 0.1971 - val_loss: 1.7707 - val_acc: 0.2240\n",
      "Epoch 3/50\n",
      " - 121s - loss: 1.7755 - acc: 0.1988 - val_loss: 1.7671 - val_acc: 0.1887\n",
      "Epoch 4/50\n",
      " - 124s - loss: 1.7748 - acc: 0.2058 - val_loss: 1.7613 - val_acc: 0.2177\n",
      "Epoch 5/50\n",
      " - 123s - loss: 1.7552 - acc: 0.2153 - val_loss: 1.7530 - val_acc: 0.2058\n",
      "Epoch 6/50\n",
      " - 122s - loss: 1.7386 - acc: 0.2368 - val_loss: 1.7422 - val_acc: 0.2460\n",
      "Epoch 7/50\n",
      " - 122s - loss: 1.7318 - acc: 0.2490 - val_loss: 1.7392 - val_acc: 0.2222\n",
      "Epoch 8/50\n",
      " - 122s - loss: 1.7070 - acc: 0.2605 - val_loss: 1.7082 - val_acc: 0.2778\n",
      "Epoch 9/50\n",
      " - 123s - loss: 1.7232 - acc: 0.2542 - val_loss: 1.6824 - val_acc: 0.2843\n",
      "Epoch 10/50\n",
      " - 124s - loss: 1.6994 - acc: 0.2793 - val_loss: 1.6674 - val_acc: 0.2801\n",
      "Epoch 11/50\n",
      " - 124s - loss: 1.6941 - acc: 0.2810 - val_loss: 1.6964 - val_acc: 0.2743\n",
      "Epoch 12/50\n",
      " - 124s - loss: 1.6937 - acc: 0.2805 - val_loss: 1.6627 - val_acc: 0.2888\n",
      "Epoch 13/50\n",
      " - 125s - loss: 1.6708 - acc: 0.2905 - val_loss: 1.6552 - val_acc: 0.3217\n",
      "Epoch 14/50\n",
      " - 123s - loss: 1.6558 - acc: 0.3037 - val_loss: 1.6478 - val_acc: 0.2831\n",
      "Epoch 15/50\n",
      " - 124s - loss: 1.6535 - acc: 0.3012 - val_loss: 1.6298 - val_acc: 0.3194\n",
      "Epoch 16/50\n",
      " - 122s - loss: 1.6189 - acc: 0.3228 - val_loss: 1.6708 - val_acc: 0.2730\n",
      "Epoch 17/50\n",
      " - 123s - loss: 1.6331 - acc: 0.3204 - val_loss: 1.6212 - val_acc: 0.3221\n",
      "Epoch 18/50\n",
      " - 123s - loss: 1.6133 - acc: 0.3278 - val_loss: 1.6679 - val_acc: 0.2901\n",
      "Epoch 19/50\n",
      " - 125s - loss: 1.6007 - acc: 0.3354 - val_loss: 1.6262 - val_acc: 0.3238\n",
      "Epoch 20/50\n",
      " - 122s - loss: 1.5973 - acc: 0.3337 - val_loss: 1.6173 - val_acc: 0.3180\n",
      "Epoch 21/50\n",
      " - 123s - loss: 1.5856 - acc: 0.3501 - val_loss: 1.6531 - val_acc: 0.2969\n",
      "Epoch 22/50\n",
      " - 124s - loss: 1.5945 - acc: 0.3503 - val_loss: 1.6313 - val_acc: 0.3191\n",
      "Epoch 23/50\n",
      " - 124s - loss: 1.5696 - acc: 0.3573 - val_loss: 1.6256 - val_acc: 0.3280\n",
      "Epoch 24/50\n",
      " - 122s - loss: 1.5879 - acc: 0.3371 - val_loss: 1.6203 - val_acc: 0.3336\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00024: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 309ms/step\n",
      "12 [1.6349485664532102, 0.2965517287367377]\n",
      "0.3 1 64 64 False\n",
      "Epoch 1/50\n",
      "FAILED\n",
      "OOM when allocating tensor with shape[35,64,504,336] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/kernel/read)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0.3 1 64 32 False\n",
      "Epoch 1/50\n",
      " - 126s - loss: 2.7457 - acc: 0.1770 - val_loss: 1.7856 - val_acc: 0.1823\n",
      "Epoch 2/50\n",
      " - 124s - loss: 1.7839 - acc: 0.2006 - val_loss: 1.7807 - val_acc: 0.1936\n",
      "Epoch 3/50\n",
      " - 124s - loss: 1.7825 - acc: 0.1794 - val_loss: 1.7725 - val_acc: 0.2286\n",
      "Epoch 4/50\n",
      " - 126s - loss: 1.7769 - acc: 0.2041 - val_loss: 1.7731 - val_acc: 0.2110\n",
      "Epoch 5/50\n",
      " - 125s - loss: 1.7738 - acc: 0.1822 - val_loss: 1.7688 - val_acc: 0.2131\n",
      "Epoch 6/50\n",
      " - 123s - loss: 1.7752 - acc: 0.2103 - val_loss: 1.7715 - val_acc: 0.2153\n",
      "Epoch 7/50\n",
      " - 124s - loss: 1.7748 - acc: 0.1891 - val_loss: 1.7672 - val_acc: 0.1931\n",
      "Epoch 8/50\n",
      " - 123s - loss: 1.7706 - acc: 0.2097 - val_loss: 1.7662 - val_acc: 0.2287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      " - 123s - loss: 1.7651 - acc: 0.2178 - val_loss: 1.7748 - val_acc: 0.1911\n",
      "Epoch 10/50\n",
      " - 123s - loss: 1.7729 - acc: 0.1927 - val_loss: 1.7698 - val_acc: 0.2007\n",
      "Epoch 11/50\n",
      " - 124s - loss: 1.7801 - acc: 0.1776 - val_loss: 1.7711 - val_acc: 0.2186\n",
      "Epoch 12/50\n",
      " - 124s - loss: 1.7747 - acc: 0.1869 - val_loss: 1.7676 - val_acc: 0.2245\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00012: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 6s 375ms/step\n",
      "13 [1.7652291883609685, 0.13183279951189875]\n",
      "0.3 1 64 16 False\n",
      "Epoch 1/50\n",
      " - 128s - loss: 2.0961 - acc: 0.1991 - val_loss: 1.7845 - val_acc: 0.2308\n",
      "Epoch 2/50\n",
      " - 123s - loss: 1.7815 - acc: 0.1984 - val_loss: 1.7780 - val_acc: 0.1657\n",
      "Epoch 3/50\n",
      " - 123s - loss: 1.7772 - acc: 0.1980 - val_loss: 1.7803 - val_acc: 0.2012\n",
      "Epoch 4/50\n",
      " - 125s - loss: 1.7764 - acc: 0.1960 - val_loss: 1.7805 - val_acc: 0.1919\n",
      "Epoch 5/50\n",
      " - 125s - loss: 1.7734 - acc: 0.1998 - val_loss: 1.7751 - val_acc: 0.2012\n",
      "Epoch 6/50\n",
      " - 122s - loss: 1.7729 - acc: 0.1816 - val_loss: 1.7791 - val_acc: 0.1969\n",
      "Epoch 7/50\n",
      " - 123s - loss: 1.7738 - acc: 0.2050 - val_loss: 1.7654 - val_acc: 0.2223\n",
      "Epoch 8/50\n",
      " - 124s - loss: 1.7648 - acc: 0.1845 - val_loss: 1.7719 - val_acc: 0.2006\n",
      "Epoch 9/50\n",
      " - 124s - loss: 1.7679 - acc: 0.2119 - val_loss: 1.7689 - val_acc: 0.2028\n",
      "Epoch 10/50\n",
      " - 124s - loss: 1.7698 - acc: 0.1916 - val_loss: 1.7678 - val_acc: 0.2119\n",
      "Epoch 11/50\n",
      " - 125s - loss: 1.7707 - acc: 0.2060 - val_loss: 1.7707 - val_acc: 0.1992\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00011: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 333ms/step\n",
      "14 [1.7781225616131686, 0.1591695535502632]\n",
      "0.3 1 32 64 False\n",
      "Epoch 1/50\n",
      " - 129s - loss: 1.9671 - acc: 0.1825 - val_loss: 1.7876 - val_acc: 0.1693\n",
      "Epoch 2/50\n",
      " - 125s - loss: 1.7828 - acc: 0.2047 - val_loss: 1.7806 - val_acc: 0.2086\n",
      "Epoch 3/50\n",
      " - 125s - loss: 1.7821 - acc: 0.2050 - val_loss: 1.7793 - val_acc: 0.2196\n",
      "Epoch 4/50\n",
      " - 126s - loss: 1.7805 - acc: 0.1891 - val_loss: 1.7753 - val_acc: 0.2012\n",
      "Epoch 5/50\n",
      " - 126s - loss: 1.7762 - acc: 0.1661 - val_loss: 1.7757 - val_acc: 0.2176\n",
      "Epoch 6/50\n",
      " - 126s - loss: 1.7777 - acc: 0.2021 - val_loss: 1.7709 - val_acc: 0.2166\n",
      "Epoch 7/50\n",
      " - 125s - loss: 1.7765 - acc: 0.1922 - val_loss: 1.7724 - val_acc: 0.2136\n",
      "Epoch 8/50\n",
      " - 128s - loss: 1.7736 - acc: 0.1945 - val_loss: 1.7723 - val_acc: 0.1984\n",
      "Epoch 9/50\n",
      " - 127s - loss: 1.7719 - acc: 0.1931 - val_loss: 1.7720 - val_acc: 0.1991\n",
      "Epoch 10/50\n",
      "FAILED\n",
      "OOM when allocating tensor with shape[37,64,504,336] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/conv2d_1/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/kernel/read)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[{{node loss/mul/_97}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_681_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0.3 1 32 32 False\n",
      "Epoch 1/50\n",
      " - 128s - loss: 1.8985 - acc: 0.1864 - val_loss: 1.7862 - val_acc: 0.2045\n",
      "Epoch 2/50\n",
      " - 124s - loss: 1.7833 - acc: 0.2006 - val_loss: 1.7804 - val_acc: 0.1971\n",
      "Epoch 3/50\n",
      " - 123s - loss: 1.7786 - acc: 0.1902 - val_loss: 1.7748 - val_acc: 0.2190\n",
      "Epoch 4/50\n",
      " - 124s - loss: 1.7804 - acc: 0.1837 - val_loss: 1.7761 - val_acc: 0.2093\n",
      "Epoch 5/50\n",
      " - 124s - loss: 1.7696 - acc: 0.2057 - val_loss: 1.7744 - val_acc: 0.2048\n",
      "Epoch 6/50\n",
      " - 123s - loss: 1.7745 - acc: 0.1765 - val_loss: 1.7706 - val_acc: 0.2143\n",
      "Epoch 7/50\n",
      " - 124s - loss: 1.7690 - acc: 0.1912 - val_loss: 1.7733 - val_acc: 0.2081\n",
      "Epoch 8/50\n",
      " - 124s - loss: 1.7664 - acc: 0.1948 - val_loss: 1.7746 - val_acc: 0.1925\n",
      "Epoch 9/50\n",
      " - 125s - loss: 1.7773 - acc: 0.1796 - val_loss: 1.7602 - val_acc: 0.2248\n",
      "Epoch 10/50\n",
      " - 125s - loss: 1.7762 - acc: 0.1821 - val_loss: 1.7729 - val_acc: 0.2233\n",
      "Epoch 11/50\n",
      " - 123s - loss: 1.7782 - acc: 0.1900 - val_loss: 1.7641 - val_acc: 0.1975\n",
      "Epoch 12/50\n",
      " - 122s - loss: 1.7686 - acc: 0.1944 - val_loss: 1.7731 - val_acc: 0.2031\n",
      "Epoch 13/50\n",
      " - 125s - loss: 1.7766 - acc: 0.1947 - val_loss: 1.7706 - val_acc: 0.2024\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00013: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 325ms/step\n",
      "15 [1.769653986226644, 0.19536424435625802]\n",
      "0.3 1 32 16 False\n",
      "Epoch 1/50\n",
      " - 126s - loss: 1.8526 - acc: 0.1936 - val_loss: 1.7845 - val_acc: 0.2095\n",
      "Epoch 2/50\n",
      " - 123s - loss: 1.7799 - acc: 0.1913 - val_loss: 1.7789 - val_acc: 0.1969\n",
      "Epoch 3/50\n",
      " - 125s - loss: 1.7799 - acc: 0.2114 - val_loss: 1.7774 - val_acc: 0.2102\n",
      "Epoch 4/50\n",
      " - 124s - loss: 1.7711 - acc: 0.1987 - val_loss: 1.7791 - val_acc: 0.1880\n",
      "Epoch 5/50\n",
      " - 123s - loss: 1.7796 - acc: 0.1853 - val_loss: 1.7789 - val_acc: 0.2136\n",
      "Epoch 6/50\n",
      " - 125s - loss: 1.7704 - acc: 0.1922 - val_loss: 1.7740 - val_acc: 0.2197\n",
      "Epoch 7/50\n",
      " - 124s - loss: 1.7747 - acc: 0.2054 - val_loss: 1.7693 - val_acc: 0.2233\n",
      "Epoch 8/50\n",
      " - 125s - loss: 1.7683 - acc: 0.1761 - val_loss: 1.7676 - val_acc: 0.2064\n",
      "Epoch 9/50\n",
      " - 124s - loss: 1.7711 - acc: 0.1797 - val_loss: 1.7651 - val_acc: 0.2134\n",
      "Epoch 10/50\n",
      " - 124s - loss: 1.7742 - acc: 0.2106 - val_loss: 1.7757 - val_acc: 0.1839\n",
      "Epoch 11/50\n",
      " - 126s - loss: 1.7784 - acc: 0.1559 - val_loss: 1.7664 - val_acc: 0.2475\n",
      "Epoch 12/50\n",
      " - 124s - loss: 1.7713 - acc: 0.1894 - val_loss: 1.7730 - val_acc: 0.2006\n",
      "Epoch 13/50\n",
      " - 124s - loss: 1.7727 - acc: 0.2068 - val_loss: 1.7648 - val_acc: 0.2135\n",
      "Epoch 14/50\n",
      " - 126s - loss: 1.7742 - acc: 0.1932 - val_loss: 1.7711 - val_acc: 0.2122\n",
      "Epoch 15/50\n",
      " - 125s - loss: 1.7733 - acc: 0.1921 - val_loss: 1.7682 - val_acc: 0.2236\n",
      "Epoch 16/50\n",
      " - 123s - loss: 1.7704 - acc: 0.1992 - val_loss: 1.7736 - val_acc: 0.2107\n",
      "Epoch 17/50\n",
      " - 125s - loss: 1.7684 - acc: 0.1932 - val_loss: 1.7651 - val_acc: 0.2296\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 330ms/step\n",
      "16 [1.7771125182106688, 0.1722972984551578]\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "\n",
    "metadata=pd.read_csv(\"file_metadata.csv\")\n",
    "\n",
    "# Parameter space\n",
    "dropout_rates = [0.5, 0.3]\n",
    "n_conv_layers = [2, 1]\n",
    "base_dense_size = [64, 32]\n",
    "n_filterses = [64, 32 ,16]\n",
    "extra_denses = [False]\n",
    "\n",
    "idx = 0\n",
    "batch_size=8\n",
    "\n",
    "for dropout in dropout_rates:\n",
    "    for n_conv in n_conv_layers:\n",
    "        for base_dense in base_dense_size:\n",
    "            for n_filters in n_filterses:\n",
    "                for extra_dense in extra_denses:\n",
    "                    print(dropout, n_conv, base_dense, n_filters, extra_dense)\n",
    "                    try:\n",
    "                        with tf.Graph().as_default():\n",
    "                            with tf.Session() as sess:\n",
    "                                # Single row df for each result, intended to be dask-read into a large df\n",
    "                                df = pd.DataFrame(columns=[\"dropout\", \"n_conv\", \"base_dense\", \"n_filters\", \"loss\", \"acc\"])\n",
    "                                vals = [dropout, n_conv, base_dense, n_filters]\n",
    "\n",
    "                                # Build this model\n",
    "                                md = build_2D_model_test((513,345, 1), 6,  \n",
    "                                                         dropout=dropout, \n",
    "                                                         n_conv=n_conv,\n",
    "                                                         base_dense=base_dense,\n",
    "                                                         n_filters=n_filters,\n",
    "                                                         extra_dense=extra_dense)\n",
    "\n",
    "                                # Create generators for data streams\n",
    "                                train_gen = batch_gen(train_meta, batch_size)\n",
    "                                val_gen = batch_gen(val_meta, batch_size)\n",
    "                                test_gen = batch_gen(test_meta, batch_size)\n",
    "\n",
    "                                md.fit_generator(train_gen, \n",
    "                                                 steps_per_epoch=len(train_meta)// batch_size,\n",
    "                                                 validation_data=val_gen, \n",
    "                                                 validation_steps=len(train_meta)// batch_size,\n",
    "                                                 epochs = 50, \n",
    "                                                 verbose=2, \n",
    "                                                 callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                     min_delta=0,\n",
    "                                                     patience=4,\n",
    "                                                     verbose=1, mode='auto', restore_best_weights=True)])\n",
    "\n",
    "                                res = md.evaluate_generator(test_gen, steps = 15, verbose=1)\n",
    "\n",
    "                                # Memory management\n",
    "                                gc.collect()\n",
    "\n",
    "                                vals.extend(res)\n",
    "\n",
    "                                df.loc[idx] = vals\n",
    "                                df.to_parquet(\"res/{}-{}-{}-{}-{}.parquet\".format(dropout, n_conv, base_dense, n_filters, extra_dense))\n",
    "                                print(idx, res)\n",
    "                                idx += 1\n",
    "                                md.save(\"md/{}-{}-{}-{}-{}.md\".format(dropout, n_conv, base_dense, n_filters, extra_dense))\n",
    "                    except Exception as e:\n",
    "                        print(\"FAILED\")\n",
    "                        print(str(e))\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(base_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 4 32 32 False\n",
      "Epoch 1/50\n",
      " - 127s - loss: 1.8084 - acc: 0.1871 - val_loss: 1.7798 - val_acc: 0.1625\n",
      "Epoch 2/50\n",
      " - 122s - loss: 1.7842 - acc: 0.1829 - val_loss: 1.7786 - val_acc: 0.2069\n",
      "Epoch 3/50\n",
      " - 123s - loss: 1.7680 - acc: 0.1987 - val_loss: 1.7781 - val_acc: 0.2042\n",
      "Epoch 4/50\n",
      " - 124s - loss: 1.7727 - acc: 0.1952 - val_loss: 1.7693 - val_acc: 0.2004\n",
      "Epoch 5/50\n",
      " - 129s - loss: 1.7743 - acc: 0.2001 - val_loss: 1.7645 - val_acc: 0.2250\n",
      "Epoch 6/50\n",
      " - 124s - loss: 1.7841 - acc: 0.1978 - val_loss: 1.7758 - val_acc: 0.1952\n",
      "Epoch 7/50\n",
      " - 125s - loss: 1.7757 - acc: 0.1759 - val_loss: 1.7760 - val_acc: 0.2097\n",
      "Epoch 8/50\n",
      " - 125s - loss: 1.7664 - acc: 0.2015 - val_loss: 1.7711 - val_acc: 0.2193\n",
      "Epoch 9/50\n",
      " - 126s - loss: 1.7599 - acc: 0.2103 - val_loss: 1.7694 - val_acc: 0.2269\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00009: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 321ms/step\n",
      "0 [1.7725615344113774, 0.19097222403312722]\n",
      "0.5 4 32 16 False\n",
      "Epoch 1/50\n",
      " - 127s - loss: 1.8020 - acc: 0.1838 - val_loss: 1.7804 - val_acc: 0.2235\n",
      "Epoch 2/50\n",
      " - 124s - loss: 1.7873 - acc: 0.1995 - val_loss: 1.7752 - val_acc: 0.2185\n",
      "Epoch 3/50\n",
      " - 124s - loss: 1.7809 - acc: 0.1764 - val_loss: 1.7673 - val_acc: 0.2290\n",
      "Epoch 4/50\n",
      " - 124s - loss: 1.7682 - acc: 0.2040 - val_loss: 1.7687 - val_acc: 0.2169\n",
      "Epoch 5/50\n",
      " - 125s - loss: 1.7739 - acc: 0.1974 - val_loss: 1.7661 - val_acc: 0.2120\n",
      "Epoch 6/50\n",
      " - 124s - loss: 1.7722 - acc: 0.1906 - val_loss: 1.7665 - val_acc: 0.2017\n",
      "Epoch 7/50\n",
      " - 123s - loss: 1.7742 - acc: 0.1973 - val_loss: 1.7697 - val_acc: 0.2151\n",
      "Epoch 8/50\n",
      " - 124s - loss: 1.7702 - acc: 0.2078 - val_loss: 1.7722 - val_acc: 0.2009\n",
      "Epoch 9/50\n",
      " - 125s - loss: 1.7733 - acc: 0.1956 - val_loss: 1.7699 - val_acc: 0.2066\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00009: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 332ms/step\n",
      "1 [1.7582891374234333, 0.16887417521993844]\n",
      "0.5 4 32 8 False\n",
      "Epoch 1/50\n",
      " - 127s - loss: 1.7977 - acc: 0.2050 - val_loss: 1.7776 - val_acc: 0.2096\n",
      "Epoch 2/50\n",
      " - 124s - loss: 1.7829 - acc: 0.1874 - val_loss: 1.7767 - val_acc: 0.1986\n",
      "Epoch 3/50\n",
      " - 122s - loss: 1.7821 - acc: 0.1935 - val_loss: 1.7740 - val_acc: 0.2054\n",
      "Epoch 4/50\n",
      " - 125s - loss: 1.7686 - acc: 0.1934 - val_loss: 1.7711 - val_acc: 0.2084\n",
      "Epoch 5/50\n",
      " - 123s - loss: 1.7677 - acc: 0.1847 - val_loss: 1.7761 - val_acc: 0.2152\n",
      "Epoch 6/50\n",
      " - 125s - loss: 1.7814 - acc: 0.1784 - val_loss: 1.7742 - val_acc: 0.2063\n",
      "Epoch 7/50\n",
      " - 124s - loss: 1.7724 - acc: 0.1853 - val_loss: 1.7723 - val_acc: 0.2127\n",
      "Epoch 8/50\n",
      " - 123s - loss: 1.7745 - acc: 0.1986 - val_loss: 1.7706 - val_acc: 0.1987\n",
      "Epoch 9/50\n",
      " - 123s - loss: 1.7707 - acc: 0.2107 - val_loss: 1.7664 - val_acc: 0.2038\n",
      "Epoch 10/50\n",
      " - 123s - loss: 1.7724 - acc: 0.1906 - val_loss: 1.7654 - val_acc: 0.2011\n",
      "Epoch 11/50\n",
      " - 125s - loss: 1.7722 - acc: 0.2087 - val_loss: 1.7687 - val_acc: 0.2055\n",
      "Epoch 12/50\n",
      " - 127s - loss: 1.7665 - acc: 0.1927 - val_loss: 1.7539 - val_acc: 0.2343\n",
      "Epoch 13/50\n",
      " - 127s - loss: 1.7700 - acc: 0.2028 - val_loss: 1.7696 - val_acc: 0.2116\n",
      "Epoch 14/50\n",
      " - 128s - loss: 1.7717 - acc: 0.2075 - val_loss: 1.7668 - val_acc: 0.2163\n",
      "Epoch 15/50\n",
      " - 128s - loss: 1.7671 - acc: 0.1969 - val_loss: 1.7686 - val_acc: 0.2184\n",
      "Epoch 16/50\n",
      " - 127s - loss: 1.7674 - acc: 0.1970 - val_loss: 1.7777 - val_acc: 0.2086\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00016: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 364ms/step\n",
      "2 [1.740797411244235, 0.2585669795161467]\n",
      "0.5 4 16 32 False\n",
      "Epoch 1/50\n",
      " - 131s - loss: 1.8052 - acc: 0.1723 - val_loss: 1.7829 - val_acc: 0.1946\n",
      "Epoch 2/50\n",
      " - 128s - loss: 1.7821 - acc: 0.2015 - val_loss: 1.7792 - val_acc: 0.2197\n",
      "Epoch 3/50\n",
      " - 127s - loss: 1.7776 - acc: 0.1959 - val_loss: 1.7671 - val_acc: 0.2430\n",
      "Epoch 4/50\n",
      " - 128s - loss: 1.7803 - acc: 0.1898 - val_loss: 1.7648 - val_acc: 0.2300\n",
      "Epoch 5/50\n",
      " - 126s - loss: 1.7866 - acc: 0.1898 - val_loss: 1.7689 - val_acc: 0.2178\n",
      "Epoch 6/50\n",
      " - 130s - loss: 1.7658 - acc: 0.1975 - val_loss: 1.7727 - val_acc: 0.2102\n",
      "Epoch 7/50\n",
      " - 128s - loss: 1.7735 - acc: 0.2145 - val_loss: 1.7795 - val_acc: 0.2033\n",
      "Epoch 8/50\n",
      " - 127s - loss: 1.7638 - acc: 0.2083 - val_loss: 1.7650 - val_acc: 0.2304\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00008: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 329ms/step\n",
      "3 [1.7917990655534797, 0.12152778116675715]\n",
      "0.5 4 16 16 False\n",
      "Epoch 1/50\n",
      " - 131s - loss: 1.8185 - acc: 0.1841 - val_loss: 1.7849 - val_acc: 0.1497\n",
      "Epoch 2/50\n",
      " - 130s - loss: 1.7859 - acc: 0.1877 - val_loss: 1.7786 - val_acc: 0.1589\n",
      "Epoch 3/50\n",
      " - 126s - loss: 1.7790 - acc: 0.1801 - val_loss: 1.7791 - val_acc: 0.1829\n",
      "Epoch 4/50\n",
      " - 128s - loss: 1.7755 - acc: 0.1791 - val_loss: 1.7716 - val_acc: 0.1709\n",
      "Epoch 5/50\n",
      " - 129s - loss: 1.7701 - acc: 0.1954 - val_loss: 1.7808 - val_acc: 0.2087\n",
      "Epoch 6/50\n",
      " - 127s - loss: 1.7760 - acc: 0.1884 - val_loss: 1.7725 - val_acc: 0.2191\n",
      "Epoch 7/50\n",
      " - 126s - loss: 1.7724 - acc: 0.2045 - val_loss: 1.7737 - val_acc: 0.1988\n",
      "Epoch 8/50\n",
      " - 130s - loss: 1.7794 - acc: 0.1899 - val_loss: 1.7651 - val_acc: 0.2160\n",
      "Epoch 9/50\n",
      " - 128s - loss: 1.7719 - acc: 0.1940 - val_loss: 1.7767 - val_acc: 0.2250\n",
      "Epoch 10/50\n",
      " - 128s - loss: 1.7767 - acc: 0.1782 - val_loss: 1.7697 - val_acc: 0.2354\n",
      "Epoch 11/50\n",
      " - 128s - loss: 1.7747 - acc: 0.1848 - val_loss: 1.7694 - val_acc: 0.2187\n",
      "Epoch 12/50\n",
      " - 127s - loss: 1.7710 - acc: 0.1912 - val_loss: 1.7743 - val_acc: 0.2272\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00012: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 311ms/step\n",
      "4 [1.7758932200345126, 0.2254545479064638]\n",
      "0.5 4 16 8 False\n",
      "Epoch 1/50\n",
      " - 132s - loss: 1.8238 - acc: 0.1964 - val_loss: 1.7812 - val_acc: 0.2049\n",
      "Epoch 2/50\n",
      " - 129s - loss: 1.7805 - acc: 0.1841 - val_loss: 1.7707 - val_acc: 0.2051\n",
      "Epoch 3/50\n",
      " - 127s - loss: 1.7798 - acc: 0.1951 - val_loss: 1.7678 - val_acc: 0.2232\n",
      "Epoch 4/50\n",
      " - 129s - loss: 1.7778 - acc: 0.1926 - val_loss: 1.7700 - val_acc: 0.2249\n",
      "Epoch 5/50\n",
      " - 127s - loss: 1.7747 - acc: 0.2005 - val_loss: 1.7676 - val_acc: 0.2087\n",
      "Epoch 6/50\n",
      " - 126s - loss: 1.7769 - acc: 0.1968 - val_loss: 1.7795 - val_acc: 0.2185\n",
      "Epoch 7/50\n",
      " - 129s - loss: 1.7723 - acc: 0.1980 - val_loss: 1.7678 - val_acc: 0.2207\n",
      "Epoch 8/50\n",
      " - 127s - loss: 1.7746 - acc: 0.2021 - val_loss: 1.7680 - val_acc: 0.1976\n",
      "Epoch 9/50\n",
      " - 128s - loss: 1.7710 - acc: 0.2124 - val_loss: 1.7617 - val_acc: 0.2093\n",
      "Epoch 10/50\n",
      " - 127s - loss: 1.7703 - acc: 0.1961 - val_loss: 1.7752 - val_acc: 0.1948\n",
      "Epoch 11/50\n",
      " - 127s - loss: 1.7758 - acc: 0.1931 - val_loss: 1.7746 - val_acc: 0.2170\n",
      "Epoch 12/50\n",
      " - 128s - loss: 1.7714 - acc: 0.2014 - val_loss: 1.7629 - val_acc: 0.2240\n",
      "Epoch 13/50\n",
      " - 128s - loss: 1.7666 - acc: 0.2257 - val_loss: 1.7783 - val_acc: 0.1907\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00013: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 333ms/step\n",
      "5 [1.8112977718484813, 0.1793103471141437]\n",
      "0.5 3 32 32 False\n",
      "Epoch 1/50\n",
      " - 132s - loss: 1.8101 - acc: 0.1832 - val_loss: 1.7846 - val_acc: 0.2118\n",
      "Epoch 2/50\n",
      " - 128s - loss: 1.7839 - acc: 0.1889 - val_loss: 1.7768 - val_acc: 0.2165\n",
      "Epoch 3/50\n",
      " - 126s - loss: 1.7803 - acc: 0.1785 - val_loss: 1.7756 - val_acc: 0.1947\n",
      "Epoch 4/50\n",
      " - 130s - loss: 1.7770 - acc: 0.1915 - val_loss: 1.7721 - val_acc: 0.2281\n",
      "Epoch 5/50\n",
      " - 127s - loss: 1.7782 - acc: 0.1888 - val_loss: 1.7729 - val_acc: 0.2150\n",
      "Epoch 6/50\n",
      " - 127s - loss: 1.7679 - acc: 0.2070 - val_loss: 1.7722 - val_acc: 0.2053\n",
      "Epoch 7/50\n",
      " - 127s - loss: 1.7700 - acc: 0.1998 - val_loss: 1.7687 - val_acc: 0.2301\n",
      "Epoch 8/50\n",
      " - 127s - loss: 1.7560 - acc: 0.2376 - val_loss: 1.7547 - val_acc: 0.2252\n",
      "Epoch 9/50\n",
      " - 126s - loss: 1.7624 - acc: 0.2112 - val_loss: 1.7544 - val_acc: 0.2549\n",
      "Epoch 10/50\n",
      " - 128s - loss: 1.7542 - acc: 0.2178 - val_loss: 1.7588 - val_acc: 0.2544\n",
      "Epoch 11/50\n",
      " - 128s - loss: 1.7628 - acc: 0.2370 - val_loss: 1.7633 - val_acc: 0.2156\n",
      "Epoch 12/50\n",
      " - 127s - loss: 1.7609 - acc: 0.2384 - val_loss: 1.7516 - val_acc: 0.2188\n",
      "Epoch 13/50\n",
      " - 127s - loss: 1.7434 - acc: 0.2479 - val_loss: 1.7418 - val_acc: 0.2174\n",
      "Epoch 14/50\n",
      " - 129s - loss: 1.7526 - acc: 0.2208 - val_loss: 1.7642 - val_acc: 0.2305\n",
      "Epoch 15/50\n",
      " - 129s - loss: 1.7359 - acc: 0.2579 - val_loss: 1.7152 - val_acc: 0.2777\n",
      "Epoch 16/50\n",
      " - 128s - loss: 1.7385 - acc: 0.2443 - val_loss: 1.7296 - val_acc: 0.2713\n",
      "Epoch 17/50\n",
      " - 127s - loss: 1.7245 - acc: 0.2552 - val_loss: 1.6923 - val_acc: 0.2866\n",
      "Epoch 18/50\n",
      " - 129s - loss: 1.7281 - acc: 0.2465 - val_loss: 1.7253 - val_acc: 0.2226\n",
      "Epoch 19/50\n",
      " - 127s - loss: 1.7124 - acc: 0.2609 - val_loss: 1.6880 - val_acc: 0.3102\n",
      "Epoch 20/50\n",
      " - 128s - loss: 1.7128 - acc: 0.2556 - val_loss: 1.6935 - val_acc: 0.2740\n",
      "Epoch 21/50\n",
      " - 128s - loss: 1.7115 - acc: 0.2583 - val_loss: 1.7214 - val_acc: 0.2574\n",
      "Epoch 22/50\n",
      " - 131s - loss: 1.7096 - acc: 0.2676 - val_loss: 1.7153 - val_acc: 0.2512\n",
      "Epoch 23/50\n",
      " - 129s - loss: 1.7111 - acc: 0.2549 - val_loss: 1.6918 - val_acc: 0.2975\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00023: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 329ms/step\n",
      "6 [1.7242222393015951, 0.2369338005703293]\n",
      "0.5 3 32 16 False\n",
      "Epoch 1/50\n",
      " - 131s - loss: 1.8076 - acc: 0.1725 - val_loss: 1.7835 - val_acc: 0.2174\n",
      "Epoch 2/50\n",
      " - 128s - loss: 1.7798 - acc: 0.2021 - val_loss: 1.7720 - val_acc: 0.2112\n",
      "Epoch 3/50\n",
      " - 126s - loss: 1.7746 - acc: 0.1852 - val_loss: 1.7714 - val_acc: 0.2174\n",
      "Epoch 4/50\n",
      " - 127s - loss: 1.7726 - acc: 0.1988 - val_loss: 1.7710 - val_acc: 0.1937\n",
      "Epoch 5/50\n",
      " - 128s - loss: 1.7736 - acc: 0.1832 - val_loss: 1.7697 - val_acc: 0.1993\n",
      "Epoch 6/50\n",
      " - 129s - loss: 1.7691 - acc: 0.2083 - val_loss: 1.7624 - val_acc: 0.2382\n",
      "Epoch 7/50\n",
      " - 127s - loss: 1.7735 - acc: 0.1883 - val_loss: 1.7671 - val_acc: 0.2251\n",
      "Epoch 8/50\n",
      " - 129s - loss: 1.7654 - acc: 0.1954 - val_loss: 1.7816 - val_acc: 0.2242\n",
      "Epoch 9/50\n",
      " - 128s - loss: 1.7668 - acc: 0.1927 - val_loss: 1.7663 - val_acc: 0.2210\n",
      "Epoch 10/50\n",
      " - 127s - loss: 1.7748 - acc: 0.2052 - val_loss: 1.7685 - val_acc: 0.2247\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00010: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 319ms/step\n",
      "7 [1.809894089967432, 0.15140845062552202]\n",
      "0.5 3 32 8 False\n",
      "Epoch 1/50\n",
      " - 133s - loss: 1.8239 - acc: 0.1769 - val_loss: 1.7824 - val_acc: 0.2150\n",
      "Epoch 2/50\n",
      " - 128s - loss: 1.7813 - acc: 0.1829 - val_loss: 1.7768 - val_acc: 0.2217\n",
      "Epoch 3/50\n",
      " - 127s - loss: 1.7770 - acc: 0.1830 - val_loss: 1.7670 - val_acc: 0.2087\n",
      "Epoch 4/50\n",
      " - 128s - loss: 1.7690 - acc: 0.1921 - val_loss: 1.7746 - val_acc: 0.2202\n",
      "Epoch 5/50\n",
      " - 128s - loss: 1.7681 - acc: 0.1920 - val_loss: 1.7738 - val_acc: 0.2120\n",
      "Epoch 6/50\n",
      " - 127s - loss: 1.7793 - acc: 0.1943 - val_loss: 1.7737 - val_acc: 0.2008\n",
      "Epoch 7/50\n",
      " - 128s - loss: 1.7779 - acc: 0.1923 - val_loss: 1.7717 - val_acc: 0.2195\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00007: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 322ms/step\n",
      "8 [1.777963156931128, 0.21107266425673937]\n",
      "0.5 3 16 32 False\n",
      "Epoch 1/50\n",
      " - 132s - loss: 1.8126 - acc: 0.1936 - val_loss: 1.7815 - val_acc: 0.2121\n",
      "Epoch 2/50\n",
      " - 128s - loss: 1.7849 - acc: 0.1795 - val_loss: 1.7788 - val_acc: 0.1667\n",
      "Epoch 3/50\n",
      " - 130s - loss: 1.7802 - acc: 0.2060 - val_loss: 1.7790 - val_acc: 0.2137\n",
      "Epoch 4/50\n",
      " - 128s - loss: 1.7754 - acc: 0.1843 - val_loss: 1.7780 - val_acc: 0.2064\n",
      "Epoch 5/50\n",
      " - 129s - loss: 1.7764 - acc: 0.1959 - val_loss: 1.7723 - val_acc: 0.1874\n",
      "Epoch 6/50\n",
      " - 129s - loss: 1.7733 - acc: 0.1842 - val_loss: 1.7726 - val_acc: 0.2173\n",
      "Epoch 7/50\n",
      " - 126s - loss: 1.7741 - acc: 0.1864 - val_loss: 1.7768 - val_acc: 0.1875\n",
      "Epoch 8/50\n",
      " - 129s - loss: 1.7677 - acc: 0.2121 - val_loss: 1.7666 - val_acc: 0.2112\n",
      "Epoch 9/50\n",
      " - 129s - loss: 1.7745 - acc: 0.2008 - val_loss: 1.7804 - val_acc: 0.2148\n",
      "Epoch 10/50\n",
      " - 129s - loss: 1.7671 - acc: 0.2120 - val_loss: 1.7611 - val_acc: 0.2160\n",
      "Epoch 11/50\n",
      " - 129s - loss: 1.7668 - acc: 0.1951 - val_loss: 1.7694 - val_acc: 0.2130\n",
      "Epoch 12/50\n",
      " - 129s - loss: 1.7750 - acc: 0.2022 - val_loss: 1.7731 - val_acc: 0.2074\n",
      "Epoch 13/50\n",
      " - 129s - loss: 1.7551 - acc: 0.2085 - val_loss: 1.7787 - val_acc: 0.2141\n",
      "Epoch 14/50\n",
      " - 130s - loss: 1.7729 - acc: 0.1963 - val_loss: 1.7767 - val_acc: 0.1882\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00014: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 337ms/step\n",
      "9 [1.801630296436042, 0.22073578478201575]\n",
      "0.5 3 16 16 False\n",
      "Epoch 1/50\n",
      " - 131s - loss: 1.8125 - acc: 0.1799 - val_loss: 1.7786 - val_acc: 0.1716\n",
      "Epoch 2/50\n",
      " - 128s - loss: 1.7840 - acc: 0.1976 - val_loss: 1.7776 - val_acc: 0.2215\n",
      "Epoch 3/50\n",
      " - 127s - loss: 1.7820 - acc: 0.1794 - val_loss: 1.7716 - val_acc: 0.2390\n",
      "Epoch 4/50\n",
      " - 127s - loss: 1.7782 - acc: 0.1743 - val_loss: 1.7771 - val_acc: 0.2102\n",
      "Epoch 5/50\n",
      " - 128s - loss: 1.7727 - acc: 0.1905 - val_loss: 1.7746 - val_acc: 0.2278\n",
      "Epoch 6/50\n",
      " - 127s - loss: 1.7745 - acc: 0.1949 - val_loss: 1.7697 - val_acc: 0.2089\n",
      "Epoch 7/50\n",
      " - 127s - loss: 1.7734 - acc: 0.2042 - val_loss: 1.7765 - val_acc: 0.2082\n",
      "Epoch 8/50\n",
      " - 126s - loss: 1.7710 - acc: 0.2095 - val_loss: 1.7779 - val_acc: 0.1982\n",
      "Epoch 9/50\n",
      " - 126s - loss: 1.7719 - acc: 0.1964 - val_loss: 1.7660 - val_acc: 0.2171\n",
      "Epoch 10/50\n",
      " - 129s - loss: 1.7733 - acc: 0.1922 - val_loss: 1.7676 - val_acc: 0.2201\n",
      "Epoch 11/50\n",
      " - 129s - loss: 1.7655 - acc: 0.1879 - val_loss: 1.7715 - val_acc: 0.1914\n",
      "Epoch 12/50\n",
      " - 129s - loss: 1.7738 - acc: 0.1826 - val_loss: 1.7643 - val_acc: 0.2093\n",
      "Epoch 13/50\n",
      " - 130s - loss: 1.7708 - acc: 0.1882 - val_loss: 1.7668 - val_acc: 0.2069\n",
      "Epoch 14/50\n",
      " - 129s - loss: 1.7758 - acc: 0.1833 - val_loss: 1.7747 - val_acc: 0.2003\n",
      "Epoch 15/50\n",
      " - 129s - loss: 1.7754 - acc: 0.2060 - val_loss: 1.7728 - val_acc: 0.2011\n",
      "Epoch 16/50\n",
      " - 130s - loss: 1.7712 - acc: 0.1874 - val_loss: 1.7746 - val_acc: 0.2133\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00016: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 324ms/step\n",
      "10 [1.7856099702544133, 0.1491525430295427]\n",
      "0.5 3 16 8 False\n",
      "Epoch 1/50\n",
      " - 133s - loss: 1.8202 - acc: 0.1883 - val_loss: 1.7832 - val_acc: 0.2015\n",
      "Epoch 2/50\n",
      " - 128s - loss: 1.7814 - acc: 0.1871 - val_loss: 1.7728 - val_acc: 0.2287\n",
      "Epoch 3/50\n",
      " - 127s - loss: 1.7801 - acc: 0.1824 - val_loss: 1.7738 - val_acc: 0.2249\n",
      "Epoch 4/50\n",
      " - 128s - loss: 1.7774 - acc: 0.1806 - val_loss: 1.7764 - val_acc: 0.2072\n",
      "Epoch 5/50\n",
      " - 129s - loss: 1.7703 - acc: 0.1976 - val_loss: 1.7767 - val_acc: 0.2030\n",
      "Epoch 6/50\n",
      " - 127s - loss: 1.7745 - acc: 0.1877 - val_loss: 1.7682 - val_acc: 0.2171\n",
      "Epoch 7/50\n",
      " - 130s - loss: 1.7704 - acc: 0.2079 - val_loss: 1.7736 - val_acc: 0.2055\n",
      "Epoch 8/50\n",
      " - 128s - loss: 1.7709 - acc: 0.2006 - val_loss: 1.7687 - val_acc: 0.2027\n",
      "Epoch 9/50\n",
      " - 129s - loss: 1.7754 - acc: 0.1957 - val_loss: 1.7728 - val_acc: 0.2199\n",
      "Epoch 10/50\n",
      " - 130s - loss: 1.7789 - acc: 0.1799 - val_loss: 1.7754 - val_acc: 0.1991\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00010: early stopping\n",
      "15/15 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 5s 357ms/step\n",
      "11 [1.7555014812314993, 0.2274143349646222]\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "\n",
    "metadata=pd.read_csv(\"file_metadata.csv\")\n",
    "\n",
    "# Parameter space\n",
    "dropout_rates = [0.5]\n",
    "n_conv_layers = [4,3]\n",
    "base_dense_size = [32, 16]\n",
    "n_filterses = [32, 16 ,8]\n",
    "extra_denses = [False]\n",
    "\n",
    "idx = 0\n",
    "batch_size=8\n",
    "\n",
    "for dropout in dropout_rates:\n",
    "    for n_conv in n_conv_layers:\n",
    "        for base_dense in base_dense_size:\n",
    "            for n_filters in n_filterses:\n",
    "                for extra_dense in extra_denses:\n",
    "                    print(dropout, n_conv, base_dense, n_filters, extra_dense)\n",
    "                    try:\n",
    "                        with tf.Graph().as_default():\n",
    "                            with tf.Session() as sess:\n",
    "                                # Single row df for each result, intended to be dask-read into a large df\n",
    "                                df = pd.DataFrame(columns=[\"dropout\", \"n_conv\", \"base_dense\", \"n_filters\", \"loss\", \"acc\"])\n",
    "                                vals = [dropout, n_conv, base_dense, n_filters]\n",
    "\n",
    "                                # Build this model\n",
    "                                md = build_2D_model_test((513,345, 1), 6,  \n",
    "                                                         dropout=dropout, \n",
    "                                                         n_conv=n_conv,\n",
    "                                                         base_dense=base_dense,\n",
    "                                                         n_filters=n_filters,\n",
    "                                                         extra_dense=extra_dense)\n",
    "\n",
    "                                # Create generators for data streams\n",
    "                                train_gen = batch_gen(train_meta, batch_size)\n",
    "                                val_gen = batch_gen(val_meta, batch_size)\n",
    "                                test_gen = batch_gen(test_meta, batch_size)\n",
    "\n",
    "                                md.fit_generator(train_gen, \n",
    "                                                 steps_per_epoch=len(train_meta)// batch_size,\n",
    "                                                 validation_data=val_gen, \n",
    "                                                 validation_steps=len(train_meta)// batch_size,\n",
    "                                                 epochs = 50, \n",
    "                                                 verbose=2, \n",
    "                                                 callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                     min_delta=0,\n",
    "                                                     patience=4,\n",
    "                                                     verbose=1, mode='auto', restore_best_weights=True)])\n",
    "\n",
    "                                res = md.evaluate_generator(test_gen, steps = 15, verbose=1)\n",
    "\n",
    "                                # Memory management\n",
    "                                gc.collect()\n",
    "\n",
    "                                vals.extend(res)\n",
    "\n",
    "                                df.loc[idx] = vals\n",
    "                                df.to_parquet(\"res/{}-{}-{}-{}-{}.parquet\".format(dropout, n_conv, base_dense, n_filters, extra_dense))\n",
    "                                print(idx, res)\n",
    "                                idx += 1\n",
    "                                md.save(\"md/{}-{}-{}-{}-{}.md\".format(dropout, n_conv, base_dense, n_filters, extra_dense))\n",
    "                    except Exception as e:\n",
    "                        print(\"FAILED\")\n",
    "                        print(str(e))\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dd.read_parquet(\"res/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropout</th>\n",
       "      <th>n_conv</th>\n",
       "      <th>base_dense</th>\n",
       "      <th>n_filters</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.573454</td>\n",
       "      <td>0.348993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.634949</td>\n",
       "      <td>0.296552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.700319</td>\n",
       "      <td>0.376712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.724222</td>\n",
       "      <td>0.236934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.740797</td>\n",
       "      <td>0.258567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.744735</td>\n",
       "      <td>0.258065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.748221</td>\n",
       "      <td>0.197987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.755501</td>\n",
       "      <td>0.227414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.758289</td>\n",
       "      <td>0.168874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.762288</td>\n",
       "      <td>0.163763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.765229</td>\n",
       "      <td>0.131833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.769654</td>\n",
       "      <td>0.195364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.769694</td>\n",
       "      <td>0.112179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.771514</td>\n",
       "      <td>0.207120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.772562</td>\n",
       "      <td>0.190972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.775893</td>\n",
       "      <td>0.225455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.777113</td>\n",
       "      <td>0.172297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.777963</td>\n",
       "      <td>0.211073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.778123</td>\n",
       "      <td>0.159170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.779746</td>\n",
       "      <td>0.147368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.785495</td>\n",
       "      <td>0.159774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.785610</td>\n",
       "      <td>0.149153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.786286</td>\n",
       "      <td>0.153584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.791799</td>\n",
       "      <td>0.121528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.792161</td>\n",
       "      <td>0.229965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.794541</td>\n",
       "      <td>0.127946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.801630</td>\n",
       "      <td>0.220736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.802431</td>\n",
       "      <td>0.141844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.809894</td>\n",
       "      <td>0.151408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.811298</td>\n",
       "      <td>0.179310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dropout  n_conv  base_dense  n_filters      loss       acc\n",
       "10      0.3     2.0        64.0       16.0  1.573454  0.348993\n",
       "12      0.3     2.0        32.0       16.0  1.634949  0.296552\n",
       "3       0.5     2.0        32.0       32.0  1.700319  0.376712\n",
       "6       0.5     3.0        32.0       32.0  1.724222  0.236934\n",
       "2       0.5     4.0        32.0        8.0  1.740797  0.258567\n",
       "5       0.5     1.0        64.0       32.0  1.744735  0.258065\n",
       "8       0.5     1.0        32.0       16.0  1.748221  0.197987\n",
       "11      0.5     3.0        16.0        8.0  1.755501  0.227414\n",
       "1       0.5     4.0        32.0       16.0  1.758289  0.168874\n",
       "11      0.3     2.0        32.0       32.0  1.762288  0.163763\n",
       "13      0.3     1.0        64.0       32.0  1.765229  0.131833\n",
       "15      0.3     1.0        32.0       32.0  1.769654  0.195364\n",
       "2       0.5     2.0        64.0       16.0  1.769694  0.112179\n",
       "0       0.5     2.0        64.0       64.0  1.771514  0.207120\n",
       "0       0.5     4.0        32.0       32.0  1.772562  0.190972\n",
       "4       0.5     4.0        16.0       16.0  1.775893  0.225455\n",
       "16      0.3     1.0        32.0       16.0  1.777113  0.172297\n",
       "8       0.5     3.0        32.0        8.0  1.777963  0.211073\n",
       "14      0.3     1.0        64.0       16.0  1.778123  0.159170\n",
       "4       0.5     2.0        32.0       16.0  1.779746  0.147368\n",
       "0       0.3     2.0        64.0        1.0  1.785495  0.159774\n",
       "10      0.5     3.0        16.0       16.0  1.785610  0.149153\n",
       "6       0.5     1.0        64.0       16.0  1.786286  0.153584\n",
       "3       0.5     4.0        16.0       32.0  1.791799  0.121528\n",
       "7       0.5     1.0        32.0       32.0  1.792161  0.229965\n",
       "9       0.3     2.0        64.0       32.0  1.794541  0.127946\n",
       "9       0.5     3.0        16.0       32.0  1.801630  0.220736\n",
       "1       0.5     2.0        64.0       32.0  1.802431  0.141844\n",
       "7       0.5     3.0        32.0       16.0  1.809894  0.151408\n",
       "5       0.5     4.0        16.0        8.0  1.811298  0.179310"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.sort_values(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
